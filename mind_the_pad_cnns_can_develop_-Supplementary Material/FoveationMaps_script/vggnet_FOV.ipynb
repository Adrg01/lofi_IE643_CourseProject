{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    "import os\r\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "vggnet = torchvision.models.vgg19(pretrained=False).eval()\r\n",
    "vggnet.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def replace_pool(m, name):\r\n",
    "    for attr_str in dir(m):\r\n",
    "        target_attr = getattr(m, attr_str)\r\n",
    "        if type(target_attr) == torch.nn.MaxPool2d:\r\n",
    "            print('replaced: ', name, attr_str)\r\n",
    "            setattr(m, attr_str, torch.nn.AvgPool2d(target_attr.kernel_size, target_attr.stride, target_attr.padding, target_attr.ceil_mode))\r\n",
    "\r\n",
    "    if isinstance(m, nn.Sequential):\r\n",
    "      for i in range(len(m)):\r\n",
    "        if type(m[i]) == torch.nn.MaxPool2d:\r\n",
    "            print('replaced: ', m[i])\r\n",
    "            m[i] = torch.nn.AvgPool2d(m[i].kernel_size, m[i].stride, m[i].padding, m[i].ceil_mode)\r\n",
    "\r\n",
    "    for n, ch in m.named_children():\r\n",
    "        replace_pool(ch, n)\r\n",
    "        \r\n",
    "replace_pool(vggnet, \"model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "replaced:  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "replaced:  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "replaced:  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "replaced:  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "replaced:  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "hooks = []\r\n",
    "def apply_back_hooks(mod):\r\n",
    "  # Make ReLUs not inplace, since we may lose output gradient\r\n",
    "  if isinstance(mod, nn.ReLU):\r\n",
    "    mod.inplace = False\r\n",
    "\r\n",
    "  # Set model weights to constant for appropriate backprop\r\n",
    "  if isinstance(mod, nn.Conv2d):\r\n",
    "    mod.weight.data = (1 / mod.weight.data.shape[0])*torch.ones_like(mod.weight.data) # torch.abs(mod.weight.data)\r\n",
    "  \r\n",
    "  global hooks\r\n",
    "  # Override gradients for most modules with output gradients, \r\n",
    "  # directly propagating gradients. Some modules have additional\r\n",
    "  # gradient (batch-norm, etc.), so only replacing first\r\n",
    "  def back_hook(module, grad_input, grad_output):\r\n",
    "    if isinstance(grad_input, tuple):\r\n",
    "      return (grad_output[0],) + grad_input[1:]\r\n",
    "    return grad_output\r\n",
    "\r\n",
    "  # Override all non-conv or avg pooling modules\r\n",
    "  if not isinstance(mod, (nn.Conv2d, nn.AvgPool2d)):\r\n",
    "    hooks.append(mod.register_backward_hook(back_hook))\r\n",
    "  \r\n",
    "vggnet.apply(apply_back_hooks)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU()\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU()\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU()\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU()\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU()\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU()\n",
       "    (36): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def modify_conv_module(mod):\r\n",
    "  if isinstance(mod, torch.nn.Conv2d):\r\n",
    "    mod.padding = (0, 0)\r\n",
    "vggnet.apply(modify_conv_module)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (15): ReLU()\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (20): ReLU()\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (22): ReLU()\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (26): ReLU()\n",
       "    (27): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (29): ReLU()\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (31): ReLU()\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (33): ReLU()\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (35): ReLU()\n",
       "    (36): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "saved_layer = None\r\n",
    "def forward_hook(mod, inp, out):\r\n",
    "  global saved_layer\r\n",
    "  saved_layer = out\r\n",
    "forward_hook = vggnet.features[34].register_forward_hook(forward_hook)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# # Valid Padding\r\n",
    "# inp = torch.zeros(1,3,512,512)\r\n",
    "# inp.requires_grad = True\r\n",
    "# out = vggnet(inp)\r\n",
    "# grad_inp_no_padding = torch.autograd.grad(torch.sum(saved_layer), inp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# # Half Padding\r\n",
    "# def modify_conv_module(mod):\r\n",
    "#   if isinstance(mod, torch.nn.Conv2d):\r\n",
    "#     mod.padding = (1, 1)\r\n",
    "# vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "# inp = torch.zeros(1,3,512,512)\r\n",
    "# inp.requires_grad = True\r\n",
    "# out = vggnet(inp)\r\n",
    "# grad_inp_same_padding = torch.autograd.grad(torch.sum(saved_layer), inp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# # Full padding   \r\n",
    "# def modify_conv_module(mod):\r\n",
    "#   if isinstance(mod, torch.nn.Conv2d):\r\n",
    "#     mod.padding = (2,2)\r\n",
    "# vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "# inp = torch.zeros(1,3,512,512)\r\n",
    "# inp.requires_grad = True\r\n",
    "# out = vggnet(inp)\r\n",
    "# grad_inp_full_padding = torch.autograd.grad(torch.sum(saved_layer), inp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# # Reflect Padding\r\n",
    "# def modify_conv_module(mod):\r\n",
    "#   if isinstance(mod, torch.nn.Conv2d):\r\n",
    "#     mod.padding_mode = 'reflect'\r\n",
    "#     mod.padding = (1, 1)\r\n",
    "    \r\n",
    "# vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "# inp = torch.zeros(1,3,512,512)\r\n",
    "# inp.requires_grad = True\r\n",
    "# out = vggnet(inp)\r\n",
    "# grad_inp_reflect_padding = torch.autograd.grad(torch.sum(saved_layer), inp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Replicate Padding\r\n",
    "def modify_conv_module(mod):\r\n",
    "  if isinstance(mod, torch.nn.Conv2d):\r\n",
    "    mod.padding = (2,2)\r\n",
    "    mod.padding_mode = 'replicate'\r\n",
    "    mod.kernel_size = (5,5)\r\n",
    "    \r\n",
    "vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "inp = torch.zeros(1,3,512,512)\r\n",
    "inp.requires_grad = True\r\n",
    "out = vggnet(inp)\r\n",
    "grad_inp_replicate_padding = torch.autograd.grad(torch.sum(saved_layer), inp)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\as116\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Dilated Padding\r\n",
    "def modify_conv_module(mod):\r\n",
    "  if isinstance(mod, torch.nn.Conv2d):\r\n",
    "    mod.padding = (1,1)\r\n",
    "    mod.kernel_size = (3,3)\r\n",
    "    mod.dilation=2\r\n",
    "vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "inp = torch.zeros(1,3,512,512)\r\n",
    "inp.requires_grad = True\r\n",
    "out = vggnet(inp)\r\n",
    "grad_inp_dilated_padding = torch.autograd.grad(torch.sum(saved_layer), inp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Circular Padding\r\n",
    "def modify_conv_module(mod):\r\n",
    "  if isinstance(mod, torch.nn.Conv2d):\r\n",
    "    mod.padding = (1, 1)\r\n",
    "    mod.padding_mode = 'circular'\r\n",
    "    mod.kernel_size = (3,3)\r\n",
    "    mod.dilation=1\r\n",
    "vggnet.apply(modify_conv_module)\r\n",
    "\r\n",
    "inp = torch.zeros(1,3,512,512)\r\n",
    "inp.requires_grad = True\r\n",
    "out = vggnet(inp)\r\n",
    "grad_inp_circular_padding = torch.autograd.grad(torch.sum(saved_layer), inp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
    "\r\n",
    "fig = plt.figure(figsize = (31,10), dpi = 63)\r\n",
    "total=3\r\n",
    "# ax = fig.add_subplot(1, total, 1)\r\n",
    "# plt.imshow((grad_inp_no_padding[0][0].sum(dim=0)), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "# ax.set_title(\"Valid Padding (0, 0)\",  size=21,  pad = 15)\r\n",
    "# divider = make_axes_locatable(ax)\r\n",
    "# cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "# plt.colorbar(cax=cax)\r\n",
    "\r\n",
    "# ax = fig.add_subplot(1, total, 2)\r\n",
    "# plt.imshow((grad_inp_same_padding[0][0].sum(dim=0)), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "# plt.title(\"Same Padding (1, 1)\", {'fontsize':21}, pad = 15)\r\n",
    "# divider = make_axes_locatable(ax)\r\n",
    "# cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "# plt.colorbar(cax=cax)\r\n",
    "\r\n",
    "# ax = fig.add_subplot(1, total, 3)\r\n",
    "# plt.imshow(grad_inp_full_padding[0][0].sum(dim=0), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "# ax.set_title(\"Full Padding (2, 2)\",  size=21,  pad = 15)\r\n",
    "# divider = make_axes_locatable(ax)\r\n",
    "# cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "# cbar=plt.colorbar(cax=cax)\r\n",
    "# cbar.ax.tick_params(labelsize=10) \r\n",
    "\r\n",
    "# ax = fig.add_subplot(1, total, 4)\r\n",
    "# plt.imshow(grad_inp_reflect_padding[0][0].sum(dim=0), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "# ax.set_title(\"Reflect Padding (2, 2)\",  size=21,  pad = 15)\r\n",
    "# divider = make_axes_locatable(ax)\r\n",
    "# cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "# cbar=plt.colorbar(cax=cax)\r\n",
    "# cbar.ax.tick_params(labelsize=10) \r\n",
    "\r\n",
    "ax = fig.add_subplot(1, total, 1)\r\n",
    "plt.imshow(grad_inp_replicate_padding[0][0].sum(dim=0), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "ax.set_title(\"Replicate Padding (2, 2)\",  size=21,  pad = 15)\r\n",
    "divider = make_axes_locatable(ax)\r\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "cbar=plt.colorbar(cax=cax)\r\n",
    "cbar.ax.tick_params(labelsize=10) \r\n",
    "\r\n",
    "ax = fig.add_subplot(1, total, 2)\r\n",
    "plt.imshow(grad_inp_circular_padding[0][0].sum(dim=0), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "ax.set_title(\"Circular Padding (2, 2)\",  size=21,  pad = 15)\r\n",
    "divider = make_axes_locatable(ax)\r\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "cbar=plt.colorbar(cax=cax)\r\n",
    "cbar.ax.tick_params(labelsize=10) \r\n",
    "\r\n",
    "ax = fig.add_subplot(1, total, 3)\r\n",
    "plt.imshow(grad_inp_dilated_padding[0][0].sum(dim=0), cmap=plt.get_cmap('plasma')) # / torch.min(grad_inp[0]))[0][0]\r\n",
    "ax.set_title(\"Dilated Padding (2, 2)\",  size=21,  pad = 15)\r\n",
    "divider = make_axes_locatable(ax)\r\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\r\n",
    "cbar=plt.colorbar(cax=cax)\r\n",
    "cbar.ax.tick_params(labelsize=10) \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "35f6d45b7a1ee2f3dfd6026ab6a59891378e324129b14532f05a55490eaa27f2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}